# Gradient Boosting models


#### Introduction


TODO: explain computation, regularization methods (variance)

TODO: various weak learners (linear, lut, stump, tree with custom weak learners as splitters and leafs)

TODO: tuning methods


#### Examples


TODO: example source code on small benchmark dataset

TODO: show how to load the dataset, create the feature generators and train the model

TODO: show how to evaluate the model using nested cross-validation

TODO: model loading and saving


#### Future work

* Extend benchmark to using stochastic solvers for faster training
* Extend benchmark to export training results to files
* Generate plots (using gnuplot) to compare various configurations (e.g. weak learners, regularization methods, features)
* Support for non-parametric statistical testing
